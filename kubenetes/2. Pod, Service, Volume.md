### 4. Pod

pod 안에는 하나의 독립적인 서비스를 제공할 수 있는 Container가 있다.

Container는 서비스가 접근할 수 있도록 port를 가지고 있는데 

컨테이너가 port를 하나 이상 가질 수는 있지만 

Pod 내에서 중복된 port를 가질 수는 없다.

Pod 내에서는 localhost로 접근하 ㄹ수 있다.

Pod가 생성될 때 고유 IP주소가 할당되는데, 쿠버네티스 클러스터 내부에서만 접근할 수 있고 외부에서는 접근할 수 없다.

Pod에 문제가 생기면 삭제되는데, 이때 IP는 변경된다.

* Container

  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
  	name: pod-1
  spec:
  	containers:
  	- name: container1
  		image: tmkube/p8000
  		ports:
  		- containerPort: 8000
    - name: container2
    	image: tmkube/p8080
    	ports:
    	- containerPort: 8080
  ```

  

* Label

  * Pod뿐만 아니라 모든 오브젝트에 달 수 있음
  * 키/밸류
  * 한 파드에는 여러개의 라벨을 달 수 있음

  ```yaml
  # Pod
  apiVersion: v1
  kind: Pod
  metadata:
  	name: pod-2
  	labels:
  		type: web
  		lo:dev
  spec:
  	containers:
  	- name: container
  		image: tmkube/init
  ```

  ```yaml
  # Service
  apiVersion: v1
  kind: Service
  metadata:
  	name: svc-1
  spec:
  	selector:
  		type: web
  	ports:
  		- port: 8080
  ```

  * Service를 만들 때 selector에 있는 라벨이 달린 Pod에 연결이 됨

* Node Scheduler

  * Pod는 여러 노드 중 한 노드에 올라가야 한다.

    * 직접선택 - 라벨

      ```yaml
      apiVersion: v1
      kind: Pod
      metadata:
      	name: pod-3
      spec:
      	nodeSelector:
      		hostname: node1
        containers:
        - name: container
        	image: tmkube/init
      ```

      

    * 쿠버네티스가 자동으로 선택 - memory, cup 등을 보고 스케쥴러가 자동으로 판단

      ````yaml
      apiVersion: v1
      kind: Pod
      metadata:
      	name: pod-4
      spec:
      	containers:
      	- name: container
          image: tmkube/init
          resources:
            requests:
            	memory: 2Gi
            limits:
            	memory: 3Gi
      ````

      * 리소스는 2Gi 필요, 최대 3Gi 사용 - 
        * memory초과시 Pod 종료
        * cpu초과시 rquest 낮춤, Pod를 종료하지 않음

### 5. Service

* 자신의 ClusterIP를 갖고 있음

* 이 서비스에 Pod를 연결시켜 놓으면 Service를 통해 Pod에 접속 가능

* Pod에도 클러스터내에서 접근하는 IP가 있는데 왜 Service를 통해 접근하지?

  * Pod는 시스템 장애/성는장애로 언제나 죽을 수 있는 존재, Pod의 IP는 신뢰성이 떨어진다.
  * 하지만 Service는 사용자가 지우지 않는한 삭제/재생성되지 않는다.

* Service의 종류 - 종류에 따라서 Pod에 접근하는 방식이 다르다

  ```yaml
  apiVersion: 1
  kind: Pod
  metadata:
  	name: pod-1
  	labels:
  		app: pod
  spec:
  	containers:
  	- name: container
  		image: tmkube/app
  		ports:
  		- containerPort: 8080
  ```

  

  * ClusterIP

    * 인가된 사용자(운영자)
    * 쿠버네티스 대시보드 관리
    * Pod의 서비스 상태 디버깅
    * 쿠버네티스 클러스터 내부에서만 접근 가능
    * 클러스터 내부의 모든 오브젝트들이 접근할 수 있지만 외부에서 접근할 수 없다.

    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
    	name: svc-1
    spec:
    	selector:
    		app: pod
      ports:
      - port: 9000
    	  targetPort: 8080
    	type: ClusterIP
    ```

    

  * NodePort

    * 내부와 연결
    * 데모나 임시 연결용
    * 클러스터에 연결되어 있는 모든 Node에 port가 할당
    * 외부로 부터 어느 노드든 포트로 연결하면 Service로 연결이 된다.

    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
    	name: svc-2
    spec:
    	selector:
    		app: pod
    	ports:
    	- port: 9000
    		targetPort: 8080
    		nodePort: 30000   # 30000 - 32767
    	type: NodePort
    ```

    * externalTrafficPolicy: Local
      * 이 설정이 없는 경우 Service는 어느 노드에서 들어온 트래픽인지 상관 없이 모든 Pod에 접근 가능하지만, 이 설정이 되어있는 경우 Service에 접근한 Node에 있는 Pod에만 트래픽을 전달할 수 있다.

  * Load Balancer

    * 로드 밸런서가 트래픽을 분산시켜 준다.
    * 로드밸런스에 접근하기 위한 외부접속 IP를 할당하기 위한 플러그인을 받아 할당해야 한다.

    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
    	name: svc-3
    spec:
    	selector:
    		app: pod
    	ports:
    	- port: 9000
    		targetPort: 8080
    	type: LocalBalancer
    ```

    

### 6. Volume

* emptyDir

  * 컨테이너들끼리 공유하기 위해 볼륨 사용
  * Pod 안에 볼륨 생성, Pod가 삭제될 때 삭제되므로 일시적인 사용목적으로 사용하는 것이 좋음

  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
  	name: pod-volume-1
  spec:
  	containers:
  	- name: container1
  		image: tmkube/init
  		volumeMounts:
  		- name: empty-dir
  			mountPath: /mount1
  	- name: container2
  		image: tmkube/init
  		volumeMounts:
  		- name: empty-dir
  			mountPath: /mount2
  	volumes:
  	- name: empty-dir
  		emptyDir: {}
  ```

  

* hostPath

  * 한 호스트의 node의 path를 volume으로 사용
  * pod가 죽어도 node에 있는 데이터가 사라지지 않음
  * 문제점: pod가 죽어서 재생성 될 때 같은 노드에 재생성되지 않기 때문에 다른 노드에 생성되면 해당 volume에 연결하지 못함
  * 해결책: node가 추가 될 때마다 똑같은 이름의 경로를 만들어서 직접 node에 있는 path 끼리 mount(운영자가 직접 연결해야 함)
  * 각 노드 자신을 위해 사용되는 시스템파일, 설정 파일 등 호스트에 데이터를 읽거나 쓸 때 사용

  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: pod-volume-3
  spec:
    nodeSelector:
      kubernetes.io/hostname: k8s-node1
    containers:
    - name: container
      image: kubetm/init
      volumeMounts:
      - name: host-path
        mountPath: /mount1
    volumes:
    - name : host-path
      hostPath:
        path: /node-v    # 사전에 해당 node에 경로가 있어야 함
        type: DirectoryOrCreate
  ```

  

* PVC(Persistent Volume  Claim)/PV(Persistent Volume)

  * Pod에 영속성 있는 Volume을 적용
  * User: 서비스 담당자 / Admin: 쿠버네티스 관리자
  * 1. Admin이 PV 정의 생성 
    2. PVC 생성
    3. PV연결
    4. Pod 생성시  PVC마운팅

  ```yaml
  apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: pv-03
  spec:
    capacity:
      storage: 2G
    accessModes:
    - ReadWriteOnce
    local:
      path: /node-v
    nodeAffinity:
      required:
        nodeSelectorTerms:
        - matchExpressions:
          - {key: kubernetes.io/hostname, operator: In, values: [k8s-node1]}
  ```

  ````yaml
  apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    name: pvc-04
  spec:
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: 1G
    storageClassName: "" # 현재 만들어진 PV 중 선택
  ````

  ````yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: pod-volume-3
  spec:
    containers:
    - name: container
      image: kubetm/init
      volumeMounts:
      - name: pvc-pv
        mountPath: /mount3
    volumes:
    - name : pvc-pv
      persistentVolumeClaim:
        claimName: pvc-01
  ````

  

### 용어정리

* 컨테이너: 애플리케이션을 실제 구동 환경으로 부터 추상화할 수 있는 논리 패키징 매커니즘을 제공
* 하이퍼바이저(hypervisor): 가상화 환경을 제공하기 위해 호스트 컴퓨터에서 다수의 운영체제를 동시에 실행하기 위한 논리적 플랫폼
* 컨테이너 런타임(container-runtime)
* On-premise: 소프트웨어 등 솔루션을 클라우드 같이 원격 환경이 아닌 자체적으로 보유한 전산실 서버에 직접 설치해 운영하는 방식
* 카나리 배포: 위험을 빠르게 감지할 수 있는 배포 전략(카나리가 유독가스에 민감하여 유독가스 누출의 위험을 알리는 새)
  * 지정한 서버 또는 특정 사용자에게만 배포했다가 정상적이면 전체를 배포
  * 서버의 트래픽 일부를 신버전으로 분산하여 오류여부 확인. 트래픽을 분산할 때는 랜덤/사용자 지정 둘다 가능
  * A/B테스트 가능
  * 성능 모니터링에 유용
* 서비스 디스커버리: 클라이언트나 API 게이트웨이가 호출할 때 서비스를 찾는 매커니즘 (MSA에서 나오는 용어 인듯)